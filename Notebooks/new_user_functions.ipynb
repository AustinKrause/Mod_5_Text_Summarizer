{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/austinkrause/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/xgboost_model.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-77fa00ffb663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load in our xgb classification model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Models/xgboost_model.sav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/xgboost_model.sav'"
     ]
    }
   ],
   "source": [
    "#load in our xgb classification model\n",
    "xgb = pickle.load(open(\"Models/xgboost_model.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "    facts = requests.get(url)\n",
    "    soup = BeautifulSoup(facts.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(url):\n",
    "    #url = input('Enter a url: \\n')\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    text = []\n",
    "    for i in soup.find_all('p'):\n",
    "        text.append(i.get_text())\n",
    "\n",
    "    full_text = ' '.join(text).replace('\\xa0', '')\n",
    "#     word_count_ = full_text.count(' ')\n",
    "#     gensim_summary = summarize(full_text, word_count = word_count_/10).strip('. ').replace('.\\n', '')\n",
    "    \n",
    "#     print('\\n Full Text:\\n', full_text, '\\n\\n')\n",
    "#     print('Extractive Summary: \\n', gensim_summary)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_article_text('https://news.yahoo.com/cambodia-islands-under-threat-woman-trying-save-them-144633896.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stop words\n",
    "stops = list(set(stopwords.words('english'))) + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to remove stop words\n",
    "def remove_stops(text):\n",
    "    text_no_stops = []\n",
    "    for i in text:\n",
    "        i = i.lower()\n",
    "        if i not in stops:\n",
    "            if len(i) == 1:\n",
    "                pass\n",
    "            else:\n",
    "                text_no_stops.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return text_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    lemmatized = []\n",
    "    for word in text:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create helper function to stem each word in a list and concat the list\n",
    "def stem_list(lst):\n",
    "    stemmed_list = []\n",
    "    for i in lst:\n",
    "        stemmed_list.append(stemmer.stem(i))\n",
    "    stem_string = ' '.join(stemmed_list[:100])\n",
    "    return stem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform lemmatization, stemming, and final preprocessing\n",
    "def xgb_text_prep(sample_text):\n",
    "    tokens = word_tokenize(str(sample_text), language = 'en')\n",
    "    no_stops = remove_stops(tokens)\n",
    "    stemmed = stem_list(no_stops)\n",
    "    #stemmed = stemmed.replace('\\\\n', '')\n",
    "    return [stemmed]\n",
    "    #return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for xgboost prdiction\n",
    "def get_xgb_prediction(text):\n",
    "    preprocess = text_prep([text])\n",
    "    return xgb.predict(preprocess)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find cosine similarities between sentences\n",
    "def find_similarities(text):\n",
    "    #tokenize sentences\n",
    "    sentences = sent_tokenize(text, language = 'en')\n",
    "    \n",
    "    #vectorize sentences\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    #creat df for article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "        \n",
    "    #find cosine similarity for all sentences\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    #print(similarities)\n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary(text):\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    sort = sorted(sents_for_sum)\n",
    "    print('\\n Sentences selected:', sort)\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    print('\\n Total sentences:', len(sent_list))\n",
    "    \n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "    summary = ' '.join(sents)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_and_summary():\n",
    "    url = input('Enter a url: \\n')\n",
    "    text = get_article_text(url)\n",
    "    print('\\n')\n",
    "    xgb_preprocess = xgb_text_prep(text)\n",
    "    print('This article was placed into cluster:', xgb.predict(xgb_preprocess)[0])\n",
    "    \n",
    "    summary = build_summary(text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST ARTICLE 1 --- Supreme Court --- returns cluster 7\n",
    "https://news.yahoo.com/u-supreme-court-spurns-virginia-143413283.html;_ylt=AwrC0COK7wddIj8AGCPQtDMD;_ylu=X3oDMTEydGhybm9tBGNvbG8DYmYxBHBvcwMxBHZ0aWQDQjc2MDlfMQRzZWMDc3I-\n",
    "\n",
    "TEST ARTICLE 2 --- Donald Trump --- returns cluster 1\n",
    "https://news.yahoo.com/trump-says-d-foreigners-offer-233911079.html;_ylt=AwrC1C7Y8AddADIAoS_QtDMD;_ylu=X3oDMTEyaTE5czNyBGNvbG8DYmYxBHBvcwM0BHZ0aWQDQjc2MDlfMQRzZWMDc3I-\n",
    "\n",
    "TEST ARTICLE 3 --- Syria --- returns cluster 11\n",
    "https://news.yahoo.com/turkish-outpost-syria-shelled-syrian-080926968.html;_ylt=AwrC0wwt8gdd2WgAkwHQtDMD;_ylu=X3oDMTEydGhybm9tBGNvbG8DYmYxBHBvcwMxBHZ0aWQDQjc2MDlfMQRzZWMDc3I-\n",
    "\n",
    "TEST ARTICLE 4 --- Birth Control Health Insurance --- returns cluster 3\n",
    "https://news.yahoo.com/democrat-warren-wants-7-billion-134454649.html;_ylt=AwrC1DGZ8gddwlsAJwLQtDMD;_ylu=X3oDMTEydDBqZzZmBGNvbG8DYmYxBHBvcwMyBHZ0aWQDQjc2MDlfMQRzZWMDc3I-\n",
    "\n",
    "TEST ARTICLE 5 --- Bernie Sanders --- returns cluster 3\n",
    "https://news.yahoo.com/sanders-outline-democratic-socialism-means-045302693.html;_ylt=AwrC0CMD4whdTSoALAHQtDMD;_ylu=X3oDMTEydDBqZzZmBGNvbG8DYmYxBHBvcwMyBHZ0aWQDQjc2MDlfMQRzZWMDc3I-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a url: \n",
      "https://www.nytimes.com/2019/06/14/books/review/podcast-underland-robert-macfarlane-disappearing-earth-julia-phillips.html\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-a1c19aa1e0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_and_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-98798d7ab91a>\u001b[0m in \u001b[0;36mxgb_and_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxgb_preprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_text_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This article was placed into cluster:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_and_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ny_articles(url):\n",
    "    page = create_soup(url)\n",
    "    a= test_.find_all('a', href = True)\n",
    "    \n",
    "    ny_times_links = []\n",
    "    for a in test_.find_all('a', href=True):\n",
    "        ny_times_links.append(a['href'])\n",
    "    \n",
    "    ny_times_articles = []\n",
    "    for i in ny_times_links:\n",
    "        if i[0] == '/' and len(i)>2 and i.endswith('Container') == False:\n",
    "            ny_times_articles.append(i)\n",
    "    \n",
    "    article_set = set(ny_times_articles)\n",
    "    return article_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/2019/06/14/books/review/podcast-underland-robert-macfarlane-disappearing-earth-julia-phillips.html',\n",
       " '/2019/06/14/the-weekly/trump-immigration-border-separation-family.html',\n",
       " '/2019/06/18/arts/music/taylor-swift-you-need-to-calm-down-video.html',\n",
       " '/2019/06/18/books/pakistan-darra-adam-khel-library.html',\n",
       " '/2019/06/18/briefing/patrick-shanahan-orlando-taylor-swift.html',\n",
       " '/2019/06/18/business/ecb-mario-draghi-stimulus.html',\n",
       " '/2019/06/18/business/economy/global-economy-trade-war.html',\n",
       " '/2019/06/18/business/japan-work-overtime-tv-show.html',\n",
       " '/2019/06/18/dining/van-da-restaurant-review.html',\n",
       " '/2019/06/18/nyregion/central-park-five-trump.html',\n",
       " '/2019/06/18/nyregion/greenhouse-gases-ny.html',\n",
       " '/2019/06/18/opinion/androgyny-wnba-fashion.html',\n",
       " '/2019/06/18/opinion/guatemala-election.html',\n",
       " '/2019/06/18/opinion/morsi-death-egypt.html',\n",
       " '/2019/06/18/opinion/pete-chasten-buttigieg.html',\n",
       " '/2019/06/18/opinion/pregnant-athlete-sponsor-climber.html',\n",
       " '/2019/06/18/opinion/religion-race-liberalism.html',\n",
       " '/2019/06/18/opinion/republican-party-women.html',\n",
       " '/2019/06/18/opinion/sarah-sanders-trump.html',\n",
       " '/2019/06/18/opinion/tiffany-caban.html',\n",
       " '/2019/06/18/opinion/trump-2020-campaign.html',\n",
       " '/2019/06/18/opinion/trump-iran-north-korea.html',\n",
       " '/2019/06/18/us/politics/donald-trump-rally-orlando.html',\n",
       " '/2019/06/18/us/politics/patrick-shanahan-defense-secretary.html',\n",
       " '/2019/06/18/us/politics/trump-fact-check-rally.html',\n",
       " '/2019/06/18/us/politics/trump-immigration-deportations.html',\n",
       " '/2019/06/18/us/trump-falwell-endorsement-michael-cohen.html'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ny_articles('https://www.nytimes.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advertisement Subscribe: iTunes | Google Play Music | How to Listen The acclaimed British writer Robert Macfarlane, who is not altogether comfortable with the term “nature writer,” has produced books about mountains, very long walks through Britain and the language we use to describe landscapes. In his new book, “Underland,” Macfarlane goes underground. He goes beneath forest floors, and into sea caves and sinkholes, among other subterranean adventures, to probe the secrets of man’s often malign influence on the earth. “It’s pretty unnatural,” Macfarlane says on this week’s podcast. “There are reasons why very little living goes on down there. Before I went anywhere, I got a friend to take me — I just said, look, take me underground for a day in Britain, let’s go somewhere scary. I need to see if I can be in these places. And I absolutely loved it. He described me later as a ‘rat in a drain pipe.’ I think that was a compliment, I’m not sure.”  Julia Phillips visits the podcast this week to discuss her debut novel, “Disappearing Earth.” The book is set on the Kamchatka Peninsula in far eastern Russia — a place that is not easy to get to, but that Phillips visited in order to write the novel. “I was extraordinarily lucky to be treated to so kindly and hospitably by everyone I met,” Phillips says. “In Moscow, for example, an American is not an unusual thing at all. But in Kamchatka, which has a population of 400,000, I was one of I believe 10 Americans there. So I was unusual enough that if I went to a fruit stand to buy fruit, someone would say, ‘Oh, you’re the American writer who has come to stay with us and write a book about Kamchatka.’ People really went out of their way to introduce me to the region and take care of me, and I am forever grateful for that.” Also on this week’s episode, Alexandra Alter has news from the publishing world; and Dwight Garner, Parul Sehgal and Jennifer Szalai talk about the books they’ve recently reviewed. Pamela Paul is the host. Here are the books discussed by The Times’s critics this week: “Let Me Not Be Mad: My Story of Unraveling Minds” by A.K. Benjamin “Reckoning: The Epic Battle Against Sexual Abuse and Harassment” by Linda Hirshman “The Capital” by Robert Menasse We would love to hear your thoughts about this episode, and about the Book Review’s podcast in general. You can send them to books@nytimes.com. Advertisement'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_text('https://www.nytimes.com/2019/06/14/books/review/podcast-underland-robert-macfarlane-disappearing-earth-julia-phillips.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
