{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-29ada89ddaa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pip install any packages you don't have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#from nltk import word_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/translate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mribes_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_ribes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mribes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeteor_score\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmeteor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malignment_error_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \"\"\"\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaintext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorpusReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m     \u001b[0mReader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconsist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mplaintext\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mParagraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36mPlaintextCorpusReader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLazyLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpara_block_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_blankline_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "#nltk\n",
    "#pip install any packages you don't have\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_with_gensim_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1.1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In the second season finale, back in 1991, the...</td>\n",
       "      <td>And never more so than in Showtime’s new serie...</td>\n",
       "      <td>['      And never more so than in Showtime’s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>When speaking to DeepMind and Google developer...</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>['      AlphaGo’s victory isn’t a defeat for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massive attack</td>\n",
       "      <td>How a weapon against war became a weapon...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>International visitors for the event are commo...</td>\n",
       "      <td>How a weapon against war became a weapon again...</td>\n",
       "      <td>['      How a weapon against war became a weap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brain drain</td>\n",
       "      <td>Genius quietly laid off a bunch of its e...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In a post on the Genius blog at the time, co-f...</td>\n",
       "      <td>Genius quietly laid off a bunch of its enginee...</td>\n",
       "      <td>['      Genius quietly laid off a bunch of its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook takes flight</td>\n",
       "      <td>Inside the test flight of Facebook’s fir...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>But if your goal is to stay in the air for a l...</td>\n",
       "      <td>Inside the test flight of Facebook’s first int...</td>\n",
       "      <td>['      Inside the test flight of Facebook’s f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1                                  AI, the humanity!   \n",
       "2                                     Massive attack   \n",
       "3                                        Brain drain   \n",
       "4                              Facebook takes flight   \n",
       "\n",
       "                                             content  category  \\\n",
       "0        And never more so than in Showtime’s new...  Longform   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  Longform   \n",
       "2        How a weapon against war became a weapon...  Longform   \n",
       "3        Genius quietly laid off a bunch of its e...  Longform   \n",
       "4        Inside the test flight of Facebook’s fir...  Longform   \n",
       "\n",
       "                                      gensim_summary  \\\n",
       "0  In the second season finale, back in 1991, the...   \n",
       "1  When speaking to DeepMind and Google developer...   \n",
       "2  International visitors for the event are commo...   \n",
       "3  In a post on the Genius blog at the time, co-f...   \n",
       "4  But if your goal is to stay in the air for a l...   \n",
       "\n",
       "                                           first_100  \\\n",
       "0  And never more so than in Showtime’s new serie...   \n",
       "1  AlphaGo’s victory isn’t a defeat for humans — ...   \n",
       "2  How a weapon against war became a weapon again...   \n",
       "3  Genius quietly laid off a bunch of its enginee...   \n",
       "4  Inside the test flight of Facebook’s first int...   \n",
       "\n",
       "                                      sent_tokenized  \n",
       "0  ['      And never more so than in Showtime’s n...  \n",
       "1  ['      AlphaGo’s victory isn’t a defeat for h...  \n",
       "2  ['      How a weapon against war became a weap...  \n",
       "3  ['      Genius quietly laid off a bunch of its...  \n",
       "4  ['      Inside the test flight of Facebook’s f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first_100 = df.first_100.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_first_100'] = df.first_100.apply(lambda x: word_tokenize(x, language = 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "      <th>tokenized_first_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In the second season finale, back in 1991, the...</td>\n",
       "      <td>and never more so than in showtime’s new serie...</td>\n",
       "      <td>['      And never more so than in Showtime’s n...</td>\n",
       "      <td>[and, never, more, so, than, in, showtime, ’, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>When speaking to DeepMind and Google developer...</td>\n",
       "      <td>alphago’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>['      AlphaGo’s victory isn’t a defeat for h...</td>\n",
       "      <td>[alphago, ’, s, victory, isn, ’, t, a, defeat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massive attack</td>\n",
       "      <td>How a weapon against war became a weapon...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>International visitors for the event are commo...</td>\n",
       "      <td>how a weapon against war became a weapon again...</td>\n",
       "      <td>['      How a weapon against war became a weap...</td>\n",
       "      <td>[how, a, weapon, against, war, became, a, weap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brain drain</td>\n",
       "      <td>Genius quietly laid off a bunch of its e...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In a post on the Genius blog at the time, co-f...</td>\n",
       "      <td>genius quietly laid off a bunch of its enginee...</td>\n",
       "      <td>['      Genius quietly laid off a bunch of its...</td>\n",
       "      <td>[genius, quietly, laid, off, a, bunch, of, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook takes flight</td>\n",
       "      <td>Inside the test flight of Facebook’s fir...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>But if your goal is to stay in the air for a l...</td>\n",
       "      <td>inside the test flight of facebook’s first int...</td>\n",
       "      <td>['      Inside the test flight of Facebook’s f...</td>\n",
       "      <td>[inside, the, test, flight, of, facebook, ’, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1                                  AI, the humanity!   \n",
       "2                                     Massive attack   \n",
       "3                                        Brain drain   \n",
       "4                              Facebook takes flight   \n",
       "\n",
       "                                             content  category  \\\n",
       "0        And never more so than in Showtime’s new...  Longform   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  Longform   \n",
       "2        How a weapon against war became a weapon...  Longform   \n",
       "3        Genius quietly laid off a bunch of its e...  Longform   \n",
       "4        Inside the test flight of Facebook’s fir...  Longform   \n",
       "\n",
       "                                      gensim_summary  \\\n",
       "0  In the second season finale, back in 1991, the...   \n",
       "1  When speaking to DeepMind and Google developer...   \n",
       "2  International visitors for the event are commo...   \n",
       "3  In a post on the Genius blog at the time, co-f...   \n",
       "4  But if your goal is to stay in the air for a l...   \n",
       "\n",
       "                                           first_100  \\\n",
       "0  and never more so than in showtime’s new serie...   \n",
       "1  alphago’s victory isn’t a defeat for humans — ...   \n",
       "2  how a weapon against war became a weapon again...   \n",
       "3  genius quietly laid off a bunch of its enginee...   \n",
       "4  inside the test flight of facebook’s first int...   \n",
       "\n",
       "                                      sent_tokenized  \\\n",
       "0  ['      And never more so than in Showtime’s n...   \n",
       "1  ['      AlphaGo’s victory isn’t a defeat for h...   \n",
       "2  ['      How a weapon against war became a weap...   \n",
       "3  ['      Genius quietly laid off a bunch of its...   \n",
       "4  ['      Inside the test flight of Facebook’s f...   \n",
       "\n",
       "                                 tokenized_first_100  \n",
       "0  [and, never, more, so, than, in, showtime, ’, ...  \n",
       "1  [alphago, ’, s, victory, isn, ’, t, a, defeat,...  \n",
       "2  [how, a, weapon, against, war, became, a, weap...  \n",
       "3  [genius, quietly, laid, off, a, bunch, of, its...  \n",
       "4  [inside, the, test, flight, of, facebook, ’, s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(set(stopwords.words('english'))) + list(punctuation) + ['s', \"'\", 't', 'and', '\"', 'a', 'or', '/', 'in',\n",
    "                                                                    'for', '&', '-', \"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stop words\n",
    "def remove_stops(text):\n",
    "    text_no_stops = []\n",
    "    for i in text:\n",
    "        if i not in stops:\n",
    "            if len(i) == 1:\n",
    "                pass\n",
    "            else:\n",
    "                text_no_stops.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return text_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_100_no_stops'] = df['tokenized_first_100'].apply(lambda x: remove_stops(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that it worked\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize WordNetLemmatizer class\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    lemmatized = []\n",
    "    for word in text:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatize_first_100'] = df['first_100_no_stops'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatize_first_100'] = df['lemmatize_first_100'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "      <th>tokenized_first_100</th>\n",
       "      <th>first_100_no_stops</th>\n",
       "      <th>lemmatize_first_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In the second season finale, back in 1991, the...</td>\n",
       "      <td>and never more so than in showtime’s new serie...</td>\n",
       "      <td>['      And never more so than in Showtime’s n...</td>\n",
       "      <td>[and, never, more, so, than, in, showtime, ’, ...</td>\n",
       "      <td>[never, showtime, new, series, revival, spoile...</td>\n",
       "      <td>never showtime new series revival spoiler ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>When speaking to DeepMind and Google developer...</td>\n",
       "      <td>alphago’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>['      AlphaGo’s victory isn’t a defeat for h...</td>\n",
       "      <td>[alphago, ’, s, victory, isn, ’, t, a, defeat,...</td>\n",
       "      <td>[alphago, victory, defeat, humans, opportunity...</td>\n",
       "      <td>alphago victory defeat human opportunity loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massive attack</td>\n",
       "      <td>How a weapon against war became a weapon...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>International visitors for the event are commo...</td>\n",
       "      <td>how a weapon against war became a weapon again...</td>\n",
       "      <td>['      How a weapon against war became a weap...</td>\n",
       "      <td>[how, a, weapon, against, war, became, a, weap...</td>\n",
       "      <td>[weapon, war, became, weapon, web, every, year...</td>\n",
       "      <td>weapon war became weapon web every year artist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brain drain</td>\n",
       "      <td>Genius quietly laid off a bunch of its e...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In a post on the Genius blog at the time, co-f...</td>\n",
       "      <td>genius quietly laid off a bunch of its enginee...</td>\n",
       "      <td>['      Genius quietly laid off a bunch of its...</td>\n",
       "      <td>[genius, quietly, laid, off, a, bunch, of, its...</td>\n",
       "      <td>[genius, quietly, laid, bunch, engineers, surv...</td>\n",
       "      <td>genius quietly laid bunch engineer survive med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook takes flight</td>\n",
       "      <td>Inside the test flight of Facebook’s fir...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>But if your goal is to stay in the air for a l...</td>\n",
       "      <td>inside the test flight of facebook’s first int...</td>\n",
       "      <td>['      Inside the test flight of Facebook’s f...</td>\n",
       "      <td>[inside, the, test, flight, of, facebook, ’, s...</td>\n",
       "      <td>[inside, test, flight, facebook, first, intern...</td>\n",
       "      <td>inside test flight facebook first internet dro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1                                  AI, the humanity!   \n",
       "2                                     Massive attack   \n",
       "3                                        Brain drain   \n",
       "4                              Facebook takes flight   \n",
       "\n",
       "                                             content  category  \\\n",
       "0        And never more so than in Showtime’s new...  Longform   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  Longform   \n",
       "2        How a weapon against war became a weapon...  Longform   \n",
       "3        Genius quietly laid off a bunch of its e...  Longform   \n",
       "4        Inside the test flight of Facebook’s fir...  Longform   \n",
       "\n",
       "                                      gensim_summary  \\\n",
       "0  In the second season finale, back in 1991, the...   \n",
       "1  When speaking to DeepMind and Google developer...   \n",
       "2  International visitors for the event are commo...   \n",
       "3  In a post on the Genius blog at the time, co-f...   \n",
       "4  But if your goal is to stay in the air for a l...   \n",
       "\n",
       "                                           first_100  \\\n",
       "0  and never more so than in showtime’s new serie...   \n",
       "1  alphago’s victory isn’t a defeat for humans — ...   \n",
       "2  how a weapon against war became a weapon again...   \n",
       "3  genius quietly laid off a bunch of its enginee...   \n",
       "4  inside the test flight of facebook’s first int...   \n",
       "\n",
       "                                      sent_tokenized  \\\n",
       "0  ['      And never more so than in Showtime’s n...   \n",
       "1  ['      AlphaGo’s victory isn’t a defeat for h...   \n",
       "2  ['      How a weapon against war became a weap...   \n",
       "3  ['      Genius quietly laid off a bunch of its...   \n",
       "4  ['      Inside the test flight of Facebook’s f...   \n",
       "\n",
       "                                 tokenized_first_100  \\\n",
       "0  [and, never, more, so, than, in, showtime, ’, ...   \n",
       "1  [alphago, ’, s, victory, isn, ’, t, a, defeat,...   \n",
       "2  [how, a, weapon, against, war, became, a, weap...   \n",
       "3  [genius, quietly, laid, off, a, bunch, of, its...   \n",
       "4  [inside, the, test, flight, of, facebook, ’, s...   \n",
       "\n",
       "                                  first_100_no_stops  \\\n",
       "0  [never, showtime, new, series, revival, spoile...   \n",
       "1  [alphago, victory, defeat, humans, opportunity...   \n",
       "2  [weapon, war, became, weapon, web, every, year...   \n",
       "3  [genius, quietly, laid, bunch, engineers, surv...   \n",
       "4  [inside, test, flight, facebook, first, intern...   \n",
       "\n",
       "                                 lemmatize_first_100  \n",
       "0  never showtime new series revival spoiler ahea...  \n",
       "1  alphago victory defeat human opportunity loss ...  \n",
       "2  weapon war became weapon web every year artist...  \n",
       "3  genius quietly laid bunch engineer survive med...  \n",
       "4  inside test flight facebook first internet dro...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df_with_lemmings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>KMEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "      <th>tokenized_first_100</th>\n",
       "      <th>first_100_no_stops</th>\n",
       "      <th>lemmatize_first_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In the second season finale, back in 1991, the...</td>\n",
       "      <td>and never more so than in showtime’s new serie...</td>\n",
       "      <td>['      And never more so than in Showtime’s n...</td>\n",
       "      <td>[and, never, more, so, than, in, showtime, ’, ...</td>\n",
       "      <td>[never, showtime, new, series, revival, spoile...</td>\n",
       "      <td>never showtime new series revival spoiler ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>When speaking to DeepMind and Google developer...</td>\n",
       "      <td>alphago’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>['      AlphaGo’s victory isn’t a defeat for h...</td>\n",
       "      <td>[alphago, ’, s, victory, isn, ’, t, a, defeat,...</td>\n",
       "      <td>[alphago, victory, defeat, humans, opportunity...</td>\n",
       "      <td>alphago victory defeat human opportunity loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massive attack</td>\n",
       "      <td>How a weapon against war became a weapon...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>International visitors for the event are commo...</td>\n",
       "      <td>how a weapon against war became a weapon again...</td>\n",
       "      <td>['      How a weapon against war became a weap...</td>\n",
       "      <td>[how, a, weapon, against, war, became, a, weap...</td>\n",
       "      <td>[weapon, war, became, weapon, web, every, year...</td>\n",
       "      <td>weapon war became weapon web every year artist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brain drain</td>\n",
       "      <td>Genius quietly laid off a bunch of its e...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In a post on the Genius blog at the time, co-f...</td>\n",
       "      <td>genius quietly laid off a bunch of its enginee...</td>\n",
       "      <td>['      Genius quietly laid off a bunch of its...</td>\n",
       "      <td>[genius, quietly, laid, off, a, bunch, of, its...</td>\n",
       "      <td>[genius, quietly, laid, bunch, engineers, surv...</td>\n",
       "      <td>genius quietly laid bunch engineer survive med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook takes flight</td>\n",
       "      <td>Inside the test flight of Facebook’s fir...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>But if your goal is to stay in the air for a l...</td>\n",
       "      <td>inside the test flight of facebook’s first int...</td>\n",
       "      <td>['      Inside the test flight of Facebook’s f...</td>\n",
       "      <td>[inside, the, test, flight, of, facebook, ’, s...</td>\n",
       "      <td>[inside, test, flight, facebook, first, intern...</td>\n",
       "      <td>inside test flight facebook first internet dro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1                                  AI, the humanity!   \n",
       "2                                     Massive attack   \n",
       "3                                        Brain drain   \n",
       "4                              Facebook takes flight   \n",
       "\n",
       "                                             content  category  \\\n",
       "0        And never more so than in Showtime’s new...  Longform   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  Longform   \n",
       "2        How a weapon against war became a weapon...  Longform   \n",
       "3        Genius quietly laid off a bunch of its e...  Longform   \n",
       "4        Inside the test flight of Facebook’s fir...  Longform   \n",
       "\n",
       "                                      gensim_summary  \\\n",
       "0  In the second season finale, back in 1991, the...   \n",
       "1  When speaking to DeepMind and Google developer...   \n",
       "2  International visitors for the event are commo...   \n",
       "3  In a post on the Genius blog at the time, co-f...   \n",
       "4  But if your goal is to stay in the air for a l...   \n",
       "\n",
       "                                           first_100  \\\n",
       "0  and never more so than in showtime’s new serie...   \n",
       "1  alphago’s victory isn’t a defeat for humans — ...   \n",
       "2  how a weapon against war became a weapon again...   \n",
       "3  genius quietly laid off a bunch of its enginee...   \n",
       "4  inside the test flight of facebook’s first int...   \n",
       "\n",
       "                                      sent_tokenized  \\\n",
       "0  ['      And never more so than in Showtime’s n...   \n",
       "1  ['      AlphaGo’s victory isn’t a defeat for h...   \n",
       "2  ['      How a weapon against war became a weap...   \n",
       "3  ['      Genius quietly laid off a bunch of its...   \n",
       "4  ['      Inside the test flight of Facebook’s f...   \n",
       "\n",
       "                                 tokenized_first_100  \\\n",
       "0  [and, never, more, so, than, in, showtime, ’, ...   \n",
       "1  [alphago, ’, s, victory, isn, ’, t, a, defeat,...   \n",
       "2  [how, a, weapon, against, war, became, a, weap...   \n",
       "3  [genius, quietly, laid, off, a, bunch, of, its...   \n",
       "4  [inside, the, test, flight, of, facebook, ’, s...   \n",
       "\n",
       "                                  first_100_no_stops  \\\n",
       "0  [never, showtime, new, series, revival, spoile...   \n",
       "1  [alphago, victory, defeat, humans, opportunity...   \n",
       "2  [weapon, war, became, weapon, web, every, year...   \n",
       "3  [genius, quietly, laid, bunch, engineers, surv...   \n",
       "4  [inside, test, flight, facebook, first, intern...   \n",
       "\n",
       "                                 lemmatize_first_100  \n",
       "0  never showtime new series revival spoiler ahea...  \n",
       "1  alphago victory defeat human opportunity loss ...  \n",
       "2  weapon war became weapon web every year artist...  \n",
       "3  genius quietly laid bunch engineer survive med...  \n",
       "4  inside test flight facebook first internet dro...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Detect languages of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df['lemmatize_first_100'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "      <th>tokenized_first_100</th>\n",
       "      <th>first_100_no_stops</th>\n",
       "      <th>lemmatize_first_100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>84941</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  content  category  gensim_summary  first_100  sent_tokenized  \\\n",
       "language                                                                        \n",
       "en        97038    97038     84941           97038      97038           97038   \n",
       "es            4        4         2               4          4               4   \n",
       "fr           19       19        19              19         19              19   \n",
       "it            2        2         2               2          2               2   \n",
       "ko            1        1         1               1          1               1   \n",
       "\n",
       "          tokenized_first_100  first_100_no_stops  lemmatize_first_100  \n",
       "language                                                                \n",
       "en                      97038               97038                97038  \n",
       "es                          4                   4                    4  \n",
       "fr                         19                  19                   19  \n",
       "it                          2                   2                    2  \n",
       "ko                          1                   1                    1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are not english\n",
    "df = df.loc[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df_english_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_english_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['words', 'word', 'running', 'ran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to stem each word in a list and concat the list\n",
    "def stem_list(lst):\n",
    "    stemmed_list = []\n",
    "    for i in lst:\n",
    "        stemmed_list.append(stemmer.stem(i))\n",
    "    stem_string = ' '.join(stemmed_list)\n",
    "    return stem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list contained in string to a regular list so it can be stemmed\n",
    "df['stemmed'] = df[\"first_100_no_stops\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem words in list\n",
    "df['stemmed'] = df[\"stemmed\"].apply(lambda x: stem_list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that it worked\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['stemmed'].str.contains(\"archiveteam.org contain\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95790, 12)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT --- SAVE TO CSV\n",
    "#df.to_csv('df_with_stems_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4595e7ee903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CHECKPOINT --- RUN TO OPEN CSV IF STARTING WORK HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_with_stems_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#CHECKPOINT --- RUN TO OPEN CSV IF STARTING WORK HERE\n",
    "df = pd.read_csv('df_with_stems_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['stemmed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never showtim new seri reviv spoiler ahead episod season twin peak may 21st showtim brought back david lynch groundbreak tv seri twin peak fulfil propheci process second season final back 1991 spirit series-defin murder victim laura palmer told fbi special agent seri protagonist dale cooper see 25 years. clip play first episod lynch twin peak reviv remind decad fact gone laura promis',\n",
       " 'alphago victori defeat human opportun loss human man succumb machin heard alphago latest exploit last week crush world best go player confirm artifici intellig master ancient chines board game may heard news deliv doomsday terms.ther certain melancholi ke jie capitul sure 19-year-old chines prodigi declar would never lose ai follow alphago earthshak victori lee se-dol last year see onstag last week nearli bent doubl',\n",
       " 'weapon war becam weapon web everi year artist technolog enthusiast meet linz austria ar electronica festiv meetup citi downtown locat danub river festiv eye toward futur someth burn man ted confer visitor navig scientif equip led light color instal intern visitor event common enough 1998 festiv featur unlik particip pentagon.that year member art group call electron disturb theater']"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to vectorize strings and perform tf-idf transformation\n",
    "def vectorize_texts(list_of_strings):\n",
    "    print('Performing vectorization and TF/IDF transformation on texts...')\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(list_of_strings)\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_texts(num_clusters, tfidf):\n",
    "    #perform kmeans clustering for range of clusters\n",
    "    print('Beginning KMeans Clustering, number of clusters = ', num_clusters, '\\n') \n",
    "    km = KMeans(n_clusters=num_clusters, max_iter = 100, verbose = 2, n_init = 1).fit(tfidf)\n",
    "    \n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Run Clustering for range of K's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing vectorization and TF/IDF transformation on texts...\n"
     ]
    }
   ],
   "source": [
    "documents_vectorized = vectorize_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning KMeans Clustering, number of clusters =  3 \n",
      "\n",
      "Initialization complete\n",
      "Iteration  0, inertia 184870.417\n",
      "Iteration  1, inertia 94221.144\n",
      "Iteration  2, inertia 93995.573\n",
      "Iteration  3, inertia 93906.754\n",
      "Iteration  4, inertia 93877.128\n",
      "Iteration  5, inertia 93865.697\n",
      "Iteration  6, inertia 93860.637\n",
      "Iteration  7, inertia 93857.378\n",
      "Iteration  8, inertia 93854.632\n",
      "Iteration  9, inertia 93852.623\n",
      "Iteration 10, inertia 93851.569\n",
      "Iteration 11, inertia 93851.179\n",
      "Iteration 12, inertia 93850.997\n",
      "Iteration 13, inertia 93850.884\n",
      "Iteration 14, inertia 93850.831\n",
      "Iteration 15, inertia 93850.796\n",
      "Iteration 16, inertia 93850.770\n",
      "Iteration 17, inertia 93850.755\n",
      "Iteration 18, inertia 93850.748\n",
      "Iteration 19, inertia 93850.744\n",
      "Iteration 20, inertia 93850.743\n",
      "Iteration 21, inertia 93850.742\n",
      "Iteration 22, inertia 93850.741\n",
      "Iteration 23, inertia 93850.741\n",
      "Iteration 24, inertia 93850.741\n",
      "Iteration 25, inertia 93850.741\n",
      "Iteration 26, inertia 93850.740\n",
      "Iteration 27, inertia 93850.740\n",
      "Iteration 28, inertia 93850.740\n",
      "Iteration 29, inertia 93850.740\n",
      "Converged at iteration 29: center shift 0.000000e+00 within tolerance 1.097946e-09\n"
     ]
    }
   ],
   "source": [
    "kmeans3 = cluster_texts(3, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans3.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = documents_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calinski_harabaz_score(test, kmeans3.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebd8a40772c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmeans4\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcluster_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_texts' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans4= cluster_texts(documents, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans5= cluster_texts(documents, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans6= cluster_texts(documents, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans7= cluster_texts(documents, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans8= cluster_texts(documents, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans9= cluster_texts(documents, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans10= cluster_texts(documents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans11= cluster_texts(documents, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans12= cluster_texts(documents, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [kmeans3, kmeans4, kmeans5, kmeans6, kmeans7, kmeans8, kmeans9, kmeans10, kmeans11, kmeans12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = km.labels_.tolist()\n",
    "    pred = km.labels_\n",
    "    score = calinski_harabaz_score(km, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_score = []\n",
    "\n",
    "for i in k_list:\n",
    "    pred = i.labels_\n",
    "    score = calinski_harabaz_score(X_2, pred)\n",
    "    CH_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([3, 4, 5, 6, 7], CH_score)\n",
    "plt.xticks([3,4,5,6,7])\n",
    "plt.title(\"Calinski Harabaz Scores for Different Values of K\")\n",
    "plt.ylabel(\"Variance Ratio\")\n",
    "plt.xlabel(\"K=\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check Clusters for K's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_={'text':documents, 'cluster':clusters} #Creating dict having doc with the corresponding cluster number.\n",
    "frame=pd.DataFrame(dict_,index=[clusters], columns=['text','cluster']) # Converting it into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_multiple(list_of_strings, max_cluster):\n",
    "    _dict = []\n",
    "    \n",
    "    for i in range(3, max_cluster):\n",
    "        clusters = cluster_texts(list_of_strings, i)\n",
    "        \n",
    "        _dict.append(clusters)\n",
    "        \n",
    "        print('finished with clustering iteration...' + '\\n\\n')\n",
    "        if i % 3 == 0:\n",
    "            print(_dict)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters_dict = cluster_multiple(documents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_={'text':documents, 'cluster':three_clusters} #Creating dict having doc with the corresponding cluster number.\n",
    "frame=pd.DataFrame(dict_,index=[three_clusters], columns=['text','cluster']) # Converting it into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    62948\n",
       "2    21151\n",
       "0    11691\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['geniu quietli laid bunch engin surviv media compani geniu rais 56.9 million promis would one day annot entir internet lose mind januari compani quietli laid quarter staff bulk cut come engin depart post geniu blog time co-found tom lehman told employe geniu plan shift emphasi away annot platform attract top-tier investor favor becom',\n",
       "        0],\n",
       "       ['march 2015 philipp rein former aid hillari clinton us state depart reach old colleagu consult firm client rein contact capricia marshal consult us chief protocol top state depart offic act liaison foreign diplomat rein want marshal arrang meet foreign embassi dataminr compani come scrutini privaci expert servic analyz twitter data.thi piec publish partnership maplight nonprofit organ reveal influenc money politics..partnership-blurb',\n",
       "        0],\n",
       "       ['facebook face wither critic us allow fake news spread 2016 presidenti elect social network come similar scrutini germani amid concern widespread disinform campaign could impact upcom elect fals mislead news articl spread rapidli across europ recent month prompt call new legisl tougher regul social media compani reaction particularli aggress germani author recent open investig matter german govern tradit taken firmer hand us regul tech compani also',\n",
       "        0],\n",
       "       ...,\n",
       "       ['khorgo kazakhstan china largest ship compani pour billion dollar buy seaport greec maritim nation around world locat latest big foreign invest given curiou twist expand ambit china ocean ship compani nearest ocean 1,600 mile away state-own chines ship giant known cosco becam 49 percent owner past summer patch frost-cov asphalt bisect railway track line warehous landlock kazakhstan barren wilder close border china stand',\n",
       "        0],\n",
       "       ['washington republican plan repeal afford care act encount new obstacl adam opposit mani older american whose health insur premium would increas aarp alli bombard congression offic object two hous committe plan vote republican bill week law repeal group say peopl 50 60 could see premium rise 2,000 3,000 year increas 20 percent 25 percent higher current rule insur charg older adult three time charg',\n",
       "        0],\n",
       "       ['san francisco airbnb agre monday settl lawsuit citi san francisco put rest litig could hamper compani effort expand go public settlement airbnb essenti agre san francisco demand transpar host help enforc exist registr law follow compani drop lawsuit decemb new york law fine peopl illeg list home short-term rental platform taken togeth action mean airbnb clear outstand litig two biggest',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.loc[frame['cluster'] == 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 182138.018\n",
      "Iteration  1, inertia 94497.045\n",
      "Iteration  2, inertia 93417.592\n",
      "Iteration  3, inertia 93304.224\n",
      "Iteration  4, inertia 93262.398\n",
      "Iteration  5, inertia 93238.397\n",
      "Iteration  6, inertia 93218.999\n",
      "Iteration  7, inertia 93197.988\n",
      "Iteration  8, inertia 93177.286\n",
      "Iteration  9, inertia 93164.417\n",
      "Iteration 10, inertia 93159.001\n",
      "Iteration 11, inertia 93156.461\n",
      "Iteration 12, inertia 93154.782\n",
      "Iteration 13, inertia 93153.714\n",
      "Iteration 14, inertia 93152.909\n",
      "Iteration 15, inertia 93152.201\n",
      "Iteration 16, inertia 93151.592\n",
      "Iteration 17, inertia 93151.122\n",
      "Iteration 18, inertia 93150.726\n",
      "Iteration 19, inertia 93150.361\n",
      "Iteration 20, inertia 93150.076\n",
      "Iteration 21, inertia 93149.830\n",
      "Iteration 22, inertia 93149.546\n",
      "Iteration 23, inertia 93149.164\n",
      "Iteration 24, inertia 93148.527\n",
      "Iteration 25, inertia 93147.714\n",
      "Iteration 26, inertia 93147.339\n",
      "Iteration 27, inertia 93147.020\n",
      "Iteration 28, inertia 93146.748\n",
      "Iteration 29, inertia 93146.494\n",
      "Iteration 30, inertia 93146.247\n",
      "Iteration 31, inertia 93145.967\n",
      "Iteration 32, inertia 93145.735\n",
      "Iteration 33, inertia 93145.395\n",
      "Iteration 34, inertia 93145.196\n",
      "Iteration 35, inertia 93145.044\n",
      "Iteration 36, inertia 93144.870\n",
      "Iteration 37, inertia 93144.780\n",
      "Iteration 38, inertia 93144.717\n",
      "Iteration 39, inertia 93144.668\n",
      "Iteration 40, inertia 93144.603\n",
      "Iteration 41, inertia 93144.572\n",
      "Iteration 42, inertia 93144.536\n",
      "Iteration 43, inertia 93144.518\n",
      "Iteration 44, inertia 93144.505\n",
      "Iteration 45, inertia 93144.492\n",
      "Iteration 46, inertia 93144.482\n",
      "Iteration 47, inertia 93144.473\n",
      "Iteration 48, inertia 93144.464\n",
      "Iteration 49, inertia 93144.456\n",
      "Iteration 50, inertia 93144.452\n",
      "Iteration 51, inertia 93144.446\n",
      "Iteration 52, inertia 93144.442\n",
      "Iteration 53, inertia 93144.439\n",
      "Iteration 54, inertia 93144.436\n",
      "Iteration 55, inertia 93144.432\n",
      "Iteration 56, inertia 93144.429\n",
      "Iteration 57, inertia 93144.423\n",
      "Iteration 58, inertia 93144.417\n",
      "Iteration 59, inertia 93144.409\n",
      "Iteration 60, inertia 93144.401\n",
      "Iteration 61, inertia 93144.391\n",
      "Iteration 62, inertia 93144.382\n",
      "Iteration 63, inertia 93144.373\n",
      "Iteration 64, inertia 93144.366\n",
      "Iteration 65, inertia 93144.361\n",
      "Iteration 66, inertia 93144.354\n",
      "Iteration 67, inertia 93144.337\n",
      "Iteration 68, inertia 93144.325\n",
      "Iteration 69, inertia 93144.316\n",
      "Iteration 70, inertia 93144.304\n",
      "Iteration 71, inertia 93144.291\n",
      "Iteration 72, inertia 93144.280\n",
      "Iteration 73, inertia 93144.269\n",
      "Iteration 74, inertia 93144.255\n",
      "Iteration 75, inertia 93144.249\n",
      "Iteration 76, inertia 93144.244\n",
      "Iteration 77, inertia 93144.239\n",
      "Iteration 78, inertia 93144.235\n",
      "Iteration 79, inertia 93144.230\n",
      "Iteration 80, inertia 93144.224\n",
      "Iteration 81, inertia 93144.219\n",
      "Iteration 82, inertia 93144.211\n",
      "Iteration 83, inertia 93144.203\n",
      "Iteration 84, inertia 93144.194\n",
      "Iteration 85, inertia 93144.186\n",
      "Iteration 86, inertia 93144.175\n",
      "Iteration 87, inertia 93144.166\n",
      "Iteration 88, inertia 93144.158\n",
      "Iteration 89, inertia 93144.152\n",
      "Iteration 90, inertia 93144.140\n",
      "Iteration 91, inertia 93144.128\n",
      "Iteration 92, inertia 93144.117\n",
      "Iteration 93, inertia 93144.106\n",
      "Iteration 94, inertia 93144.093\n",
      "Iteration 95, inertia 93144.076\n",
      "Iteration 96, inertia 93144.056\n",
      "Iteration 97, inertia 93144.020\n",
      "Iteration 98, inertia 93143.989\n",
      "Iteration 99, inertia 93143.964\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 10 #Change it according to your data.\n",
    "km = KMeans(n_clusters=num_clusters, max_iter = 100, verbose = 2, n_init = 1)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 3, 8, 3, 3, 3, 3, 3, 0]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_={'text':documents, 'cluster':clusters} #Creating dict having doc with the corresponding cluster number.\n",
    "frame=pd.DataFrame(dict_,index=[clusters], columns=['text','cluster']) # Converting it into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    31417\n",
       "0    21876\n",
       "9    12774\n",
       "1    10159\n",
       "8     6713\n",
       "6     4363\n",
       "4     3571\n",
       "7     2857\n",
       "2     2060\n",
       "5     1248\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5],\n",
       "       ['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5],\n",
       "       ['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5],\n",
       "       ...,\n",
       "       ['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5],\n",
       "       ['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5],\n",
       "       ['main site archiv team archiveteam.org contain date inform variou project manifesto plan walkthroughs. collect contain output mani archiv team project ongo complet thank gener provid disk space internet archiv multi-terabyt dataset made avail well use wayback machin provid path back lost websit work collect grown point sub-collect type data acquir seek brows content',\n",
       "        5]], dtype=object)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.loc[frame['cluster'] == 5].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
