{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/austinkrause/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_with_gensim_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1.1'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preview of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In the second season finale, back in 1991, the...</td>\n",
       "      <td>And never more so than in Showtime’s new serie...</td>\n",
       "      <td>['      And never more so than in Showtime’s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>When speaking to DeepMind and Google developer...</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>['      AlphaGo’s victory isn’t a defeat for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massive attack</td>\n",
       "      <td>How a weapon against war became a weapon...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>International visitors for the event are commo...</td>\n",
       "      <td>How a weapon against war became a weapon again...</td>\n",
       "      <td>['      How a weapon against war became a weap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brain drain</td>\n",
       "      <td>Genius quietly laid off a bunch of its e...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>In a post on the Genius blog at the time, co-f...</td>\n",
       "      <td>Genius quietly laid off a bunch of its enginee...</td>\n",
       "      <td>['      Genius quietly laid off a bunch of its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook takes flight</td>\n",
       "      <td>Inside the test flight of Facebook’s fir...</td>\n",
       "      <td>Longform</td>\n",
       "      <td>But if your goal is to stay in the air for a l...</td>\n",
       "      <td>Inside the test flight of Facebook’s first int...</td>\n",
       "      <td>['      Inside the test flight of Facebook’s f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1                                  AI, the humanity!   \n",
       "2                                     Massive attack   \n",
       "3                                        Brain drain   \n",
       "4                              Facebook takes flight   \n",
       "\n",
       "                                             content  category  \\\n",
       "0        And never more so than in Showtime’s new...  Longform   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  Longform   \n",
       "2        How a weapon against war became a weapon...  Longform   \n",
       "3        Genius quietly laid off a bunch of its e...  Longform   \n",
       "4        Inside the test flight of Facebook’s fir...  Longform   \n",
       "\n",
       "                                      gensim_summary  \\\n",
       "0  In the second season finale, back in 1991, the...   \n",
       "1  When speaking to DeepMind and Google developer...   \n",
       "2  International visitors for the event are commo...   \n",
       "3  In a post on the Genius blog at the time, co-f...   \n",
       "4  But if your goal is to stay in the air for a l...   \n",
       "\n",
       "                                           first_100  \\\n",
       "0  And never more so than in Showtime’s new serie...   \n",
       "1  AlphaGo’s victory isn’t a defeat for humans — ...   \n",
       "2  How a weapon against war became a weapon again...   \n",
       "3  Genius quietly laid off a bunch of its enginee...   \n",
       "4  Inside the test flight of Facebook’s first int...   \n",
       "\n",
       "                                      sent_tokenized  \n",
       "0  ['      And never more so than in Showtime’s n...  \n",
       "1  ['      AlphaGo’s victory isn’t a defeat for h...  \n",
       "2  ['      How a weapon against war became a weap...  \n",
       "3  ['      Genius quietly laid off a bunch of its...  \n",
       "4  ['      Inside the test flight of Facebook’s f...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clustering --- MESSY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df.title.tolist()\n",
    "summaries = df.gensim_summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clusters = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a5c6445f8>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=pred_clusters, s=10)\n",
    "terms = vectorizer.get_feature_names()\n",
    "plt.scatter(order_centroids[:, 0], order_centroids[:, 1], c='black', s=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ast.literal_eval(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_article_sents(sentences):\n",
    "    #tune function for edge cases\n",
    "    if type(sentences) == str:\n",
    "        #if text is a list within a string:\n",
    "        if sentences[0] == '[':\n",
    "            #perform literal eval of string (take off the quotes and keep the list)\n",
    "            document = ast.literal_eval(sentences)\n",
    "            print('literal eval')\n",
    "        else:\n",
    "            #if raw text, tokenize the sentences\n",
    "            document = sent_tokenize(sentences, language = 'en')\n",
    "            print('raw text')\n",
    "    else:\n",
    "        document = sentences\n",
    "        print('already tokenize')\n",
    "    print(document[:10])\n",
    "    print(len(document))\n",
    "    \n",
    "    #vectorize sentences of the document\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    \n",
    "    #cluster sentences within document\n",
    "    k = int(np.floor(len(document)**.5))\n",
    "    print(k)\n",
    "    model = KMeans(n_clusters = k, random_state = 0) #, init = 'k-means++', max_iter = 100, n_init = 1)\n",
    "    \n",
    "    #fit model to document\n",
    "    model.fit(X)\n",
    "   \n",
    "    #create empty lists to hold ################\n",
    "    avg = []\n",
    "    closest = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        idx = np.where(model.labels_ == i)[0]\n",
    "        print(idx)\n",
    "        avg.append(np.mean(idx))\n",
    "    print(avg)\n",
    "    print(model.cluster_centers_)\n",
    "    #closest, _ = pairwise_distances_argmin_min(model.cluster_centers_, !!!!)\n",
    "\n",
    "    \n",
    "    \n",
    "    #plotting\n",
    "    centers = model.cluster_centers_ #.argsort()[:, ::-1]\n",
    "    #plt.scatter(X[:, 0], X[:, 1], c = X, s=10)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.sent_tokenized[2]\n",
    "len(ast.literal_eval(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['this is one sentence.', 'this is a second sentence.', 'this is a third.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literal eval\n",
      "['      How a weapon against war became a weapon against the web Every year, artists and technology enthusiasts meet in Linz, Austria, for the Ars Electronica Festival, a meetup in the city’s downtown, located just off the Danube River.', 'The festival is a haven for those with an eye toward the future — something between Burning Man and a TED conference, with visitors navigating scientific equipment, LED lights, and colorful installations.', 'International visitors for the event are common enough, but the 1998 festival featured an unlikely participant: the Pentagon.That year, members of an art group called the Electronic Disturbance Theater were invited to demonstrate a program called FloodNet.', 'Billed as a “virtual sit-in,” users navigated to the FloodNet website at a predetermined time, and through a simple Java tool, were directed to a targeted website that would reload constantly, every few seconds.', 'With enough people — perhaps thousands — the sit-in caused targeted websites to slow or maybe even crash, rendering them intermittently inaccessible.The ability to neutralize practically any website on demand...was a powerful new tool for global civic disobedienceThe group set three targets, picked in solidarity with Mexico’s Zapatista revolutionary movement and “against neoliberalism and the global economy”: Mexican president Ernesto Zedillo, the Frankfurt Stock Exchange, and what was then the site of the US Department of Defense, defenselink.mil.', 'The group called its protests “actions,” and this action was called SWARM — Stop the War in Mexico.The ability to neutralize practically any website on demand — what’s now commonly referred to as a distributed denial of service, or DDoS, attack — was a powerful new tool for global civic disobedience.', 'But for agencies like the Department of Defense, then facing the earliest prospects of war in the digital age, the capability posed a legitimate threat, and a dark omen of what was to come.', 'This year marks the 20th anniversary of the group’s earliest beginnings, and the questions raised by Electronic Disturbance Theater’s actions are relevant in ways its creators could not have foreseen.', 'Today the DDoS attack seems like a fait accompli of digital life: in October, a bot-powered DDoS attack shut down major sites around the internet — a caper that exposed the fragility of the internet, as the attack was soon classified as the largest of its kind ever observed.In a documentary from 2001, Ricardo Dominguez, EDT’s bespectacled, wavy-haired ringleader, explains the group’s mission in the sober, scholarly diction of a determined revolutionary: “Electronic civil disobedience is non-violent direct action online.” EDT’s precise ideological origins are murky, and not without some controversy.', 'In the 1990s, a group called Critical Art Ensemble was formed in Tallahassee, Florida.']\n",
      "77\n",
      "8\n",
      "[13 14 16 39 49 55 59]\n",
      "[ 2  6  7 10 51 58 66]\n",
      "[ 9 29 33 34 36 40 41 44 45 46 47 52 63 68 70]\n",
      "[ 1  8 11 15 20 21 26 28 32 42 48 50 54 60 65 67 72 75]\n",
      "[ 0 22 24 27 31 37 38 43 57 69 71]\n",
      "[ 5 12 53 56 61 62 73 74 76]\n",
      "[18 19 23 30 35]\n",
      "[ 3  4 17 25 64]\n",
      "[35.0, 28.571428571428573, 43.8, 38.611111111111114, 38.09090909090909, 52.44444444444444, 25.0, 22.6]\n",
      "[[0.         0.05595945 0.         ... 0.02704953 0.02704953 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.03704576 0.10522694]\n",
      " [0.         0.         0.         ... 0.02694615 0.07687971 0.14796195]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD9CAYAAAC4EtBTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEoRJREFUeJzt3X+s3fV93/Hna/Z8WZghxHbsFNvFEWyes1bqODOVgKwKNgGkxonKJMfWirYsXsKMNEXTBsqmtmyTSlSNKYCoTGmFsmSQITWzFFpCoZtM1DlcJ6TE9WXcuD9wid3rGTGTqHZJ3/vjfo3u5/pefLnn+B4ufj6ko/P9fr7vc877c/3jdb/f7znnm6pCkqQz/sawG5AkvbMYDJKkhsEgSWoYDJKkhsEgSWoYDJKkxkCCIclNSV5MMp7kzhm2jyR5rNu+P8kV3fjfTPJIkheSHEpy1yD6kSTNX9/BkGQJ8ABwM7AJ+GSSTdPKPgW8WlVXAvcC93Tj/xgYqaqfAq4G/sWZ0JAkDccg9hg2A+NVdbiqTgOPAtum1WwDHumWHwduSBKggIuTLAX+FnAa+H8D6EmSNE+DCIbLgZenrB/pxmasqao3gNeAFUyGxA+BHwB/BvxaVZ0YQE+SpHlaOoDnyAxj079nY7aazcCPgZ8ALgP2Jfm9qjp81osku4BdABdffPHVGzdu7KtpSbrQHDhw4HhVrTpX3SCC4Qiwbsr6WuCVWWqOdIeNLgVOADuA362qvwL+Isk3gR5wVjBU1R5gD0Cv16vR0dEBtC5JF44kfzqXukEcSnoOuCrJhiTLgO3A3mk1e4HbuuVbgWdq8tv7/gz4SCZdDPwsMDaAniRJ89R3MHTnDHYDTwKHgK9W1cEkdyf5WFf2MLAiyTjwOeDMW1ofAP428D0mA+a3quoP++1JkjR/WYxfu+2hJEl6+5IcqKreuer85LMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqTGQYEhyU5IXk4wnuXOG7SNJHuu2709yxZRtP53kD5IcTPJCkosG0ZMkaX76DoYkS4AHgJuBTcAnk2yaVvYp4NWquhK4F7ine+xS4L8Cn6mqDwE/B/xVvz1JkuZvEHsMm4HxqjpcVaeBR4Ft02q2AY90y48DNyQJcCPwh1X1XYCq+r9V9eMB9CRJmqdBBMPlwMtT1o90YzPWVNUbwGvACuDvAJXkySTfTvJvBtCPJKkPSwfwHJlhrOZYsxS4DviHwI+Ap5McqKqnz3qRZBewC2D9+vV9NSxJmt0g9hiOAOumrK8FXpmtpjuvcClwohv/X1V1vKp+BDwB/IOZXqSq9lRVr6p6q1atGkDbkqSZDCIYngOuSrIhyTJgO7B3Ws1e4LZu+Vbgmaoq4Engp5O8pwuMfwT80QB6kiTNU9+HkqrqjSS7mfxPfgnwm1V1MMndwGhV7QUeBr6UZJzJPYXt3WNfTfKfmQyXAp6oqq/325Mkaf4y+Yv74tLr9Wp0dHTYbVxQJiYmOHbsGKtXr8ZDedLi1J3D7Z2rzk8+6y2NjY2xZcsW1q1bx7XXXsu6devYunUrY2Njw25N0nkyiHcl6V1qbGyMa665hpMnT1JVnDp1CoCnn36aa665hv3797Nx48Yhdylp0Nxj0Kx27979ZihMVVWcPHmSO+64Y0idSTqfDAbNaGJigmefffasUDijqti3bx/Hjx9f4M4knW8Gg2Z07NgxRkZG3rJm2bJlHD16dIE6krRQDAbNaPXq1W+eU5jN6dOnWbNmzQJ1JGmhGAya0apVq7juuuuY/K7DsyXh+uuvZ+XKlQvcmaTzzWDQrO6//36WL19+VjgkYfny5dx3331D6kzS+WQwaFYbN25k//793HDDDYyMjLB8+XJGRkbYsmWLb1WV3sX8HIPe0saNG3nqqac4fvw4R48eZc2aNR4+kt7lDAbNycqVKw0E6QLhoSRJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1BhIMSW5K8mKS8SR3zrB9JMlj3fb9Sa6Ytn19kteT/OtB9CNJmr++gyHJEuAB4GZgE/DJJJumlX0KeLWqrgTuBe6Ztv1e4Hf67UWS1L9B7DFsBsar6nBVnQYeBbZNq9kGPNItPw7ckCQAST4OHAYODqAXSVKfBhEMlwMvT1k/0o3NWFNVbwCvASuSXAz8W+BXBtCHJGkABhEMmWGs5ljzK8C9VfX6OV8k2ZVkNMnoxMTEPNqUJM3F0gE8xxFg3ZT1tcArs9QcSbIUuBQ4AVwD3JrkC8B7gb9O8pdVdf/0F6mqPcAegF6vNz14JEkDMohgeA64KskG4M+B7cCOaTV7gduAPwBuBZ6pqgKuP1OQ5JeB12cKBUnSwuk7GKrqjSS7gSeBJcBvVtXBJHcDo1W1F3gY+FKScSb3FLb3+7qSpPMjk7+4Ly69Xq9GR0eH3YYkLSpJDlRV71x1fvJZktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQwGCRJDYNBktQYSDAkuSnJi0nGk9w5w/aRJI912/cnuaIb35rkQJIXuvuPDKIfSdL89R0MSZYADwA3A5uATybZNK3sU8CrVXUlcC9wTzd+HPj5qvop4DbgS/32I0nqzyD2GDYD41V1uKpOA48C26bVbAMe6ZYfB25Ikqr6TlW90o0fBC5KMjKAniRJ8zSIYLgceHnK+pFubMaaqnoDeA1YMa3mF4DvVNWpAfQkSZqnpQN4jswwVm+nJsmHmDy8dOOsL5LsAnYBrF+//u13KUmak0HsMRwB1k1ZXwu8MltNkqXApcCJbn0t8NvAL1bV92d7karaU1W9quqtWrVqAG1LkmYyiGB4DrgqyYYky4DtwN5pNXuZPLkMcCvwTFVVkvcCXwfuqqpvDqAXSVKf+g6G7pzBbuBJ4BDw1ao6mOTuJB/ryh4GViQZBz4HnHlL627gSuDfJ3m+u72/354kSfOXqumnA975er1ejY6ODrsNSVpUkhyoqt656vzksySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGgySpYTBIkhoGg+bkwQcf5BOf+AQPPvjgsFvRIjcxMcH3vvc9JiYmht2KZjGQYEhyU5IXk4wnuXOG7SNJHuu2709yxZRtd3XjLyb56CD60eDs3LmTJNx+++187Wtf4/bbbycJO3fuHHZrWmTGxsbYsmUL69at49prr2XdunVs3bqVsbGxYbem6aqqrxuwBPg+8EFgGfBdYNO0mtuBX++WtwOPdcubuvoRYEP3PEvO9ZpXX3116fzbsWNHAbPeduzYMewWtUgcOnSoLrnkkkrS/B1KUpdcckkdOnRo2C1eEIDRmsP/64PYY9gMjFfV4ao6DTwKbJtWsw14pFt+HLghSbrxR6vqVFX9MTDePZ/eAb7yla/0tV06Y/fu3Zw8efLML4pvqipOnjzJHXfcMaTONJNBBMPlwMtT1o90YzPWVNUbwGvAijk+VkMw13MJDz300HnuRIvdxMQEzz777FmhcEZVsW/fPo4fP77AnWk2gwiGzDA2/W/AbDVzeezkEyS7kowmGfWk1fn3jW98Y051TzzxxHnuRIvdsWPHGBkZecuaZcuWcfTo0QXqSOcyiGA4Aqybsr4WeGW2miRLgUuBE3N8LABVtaeqelXVW7Vq1QDa1lu58cYb51R3yy23nOdOtNitXr2aU6dOvWXN6dOnWbNmzQJ1pHMZRDA8B1yVZEOSZUyeXN47rWYvcFu3fCvwTHciZC+wvXvX0gbgKuBbA+hJffrsZz87p7pPf/rT57kTLXarVq3iuuuuY/K04tmScP3117Ny5coF7kyz6TsYunMGu4EngUPAV6vqYJK7k3ysK3sYWJFkHPgccGf32IPAV4E/An4X+JdV9eN+e9Jg7Nixo6/t0hn3338/y5cvPysckrB8+XLuu+++IXWmmWS2E0LvZL1er0ZHR4fdxgVh586dM777aMeOHXz5y18eQkdarMbGxrjjjjvYt28fy5Yt4/Tp03z4wx/mi1/8Ihs3bhx2exeEJAeqqnfOOoNBc/HQQw/xxBNPcMstt3j4SH05fvw4R48eZc2aNR4+WmAGgySpMddg8LuSJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEmNvoIhyfuSPJXkpe7+slnqbutqXkpyWzf2niRfTzKW5GCSX+2nF0nSYPS7x3An8HRVXQU83a03krwP+CXgGmAz8EtTAuTXqmoj8DPAtUlu7rMfSVKf+g2GbcAj3fIjwMdnqPko8FRVnaiqV4GngJuq6kdV9fsAVXUa+Dawts9+JEl96jcYVlfVDwC6+/fPUHM58PKU9SPd2JuSvBf4eSb3OiRJQ7T0XAVJfg9YM8Omz8/xNTLDWE15/qXAfwO+WFWH36KPXcAugPXr18/xpSVJb9c5g6Gqtsy2LcmxJB+oqh8k+QDwFzOUHQF+bsr6WuB/TlnfA7xUVf/lHH3s6Wrp9Xr1VrWSpPnr91DSXuC2bvk24H/MUPMkcGOSy7qTzjd2YyT5j8ClwL/qsw9J0oD0Gwy/CmxN8hKwtVsnSS/JbwBU1QngPwDPdbe7q+pEkrVMHo7aBHw7yfNJ/nmf/UiS+pSqxXdUptfr1ejo6LDbkKRFJcmBquqdq85PPkuSGgaDJKlhMEiSGovyHEOSCeBPh/TyK4HjQ3rtYXPuFybn/u7xk1W16lxFizIYhinJ6FxO3rwbOXfnfqG5UOfuoSRJUsNgkCQ1DIa3b8+wGxgi535hcu4XGM8xSJIa7jFIkhoGwzT9XK60G/9PSV5O8vrCdd2fJDcleTHJeJKZrsI3kuSxbvv+JFdM2XZXN/5iko8uZN+DMN+5J1mR5PeTvJ7k/oXuexD6mPvWJAeSvNDdf2She+9XH3Pf3H2v2/NJvpvkEwvd+4KoKm9TbsAXgDu75TuBe2aoeR9wuLu/rFu+rNv2s8AHgNeHPZc5zncJ8H3gg8Ay4LvApmk1twO/3i1vBx7rljd19SPAhu55lgx7Tgs094uB64DPAPcPey4LPPefAX6iW/77wJ8Pez4LOPf3AEu75TOXGlg67DkN+uYew9nmfblSgKr639Vd1W6R2AyMV9XhmrzE6qNM/gymmvozeRy4IUm68Uer6lRV/TEw3j3fYjHvuVfVD6vqWeAvF67dgepn7t+pqle68YPARUlGFqTrwehn7j+qqje68YuYctGxdxOD4WwDuVzpIjKXubxZ0/2jeA1YMcfHvpP1M/fFblBz/wXgO1V16jz1eT70Nfck1yQ5CLwAfGZKULxrnPMKbu9G5/typYvMXOYyW81i/zn0M/fFru+5J/kQcA+TF99aTPqae1XtBz6U5O8BjyT5naparHuOM7ogg6HO/+VKF5MjwLop62uBV2apOdJdo/tS4MQcH/tO1s/cF7u+5t5daOu3gV+squ+f/3YHaiB/7lV1KMkPmTzP8q66QIyHks7W1+VKF6HngKuSbEiyjMkTbXun1Uz9mdwKPFOTZ9/2Atu7d3BsAK4CvrVAfQ9CP3Nf7OY99yTvBb4O3FVV31ywjgenn7lv6IKCJD8J/F3gTxam7QU07LPf77Qbk8cRnwZe6u7f1433gN+YUvfPmDzZOg780ynjX2Dyt42/7u5/edhzmsOcbwH+D5Pv1Ph8N3Y38LFu+SLgv3dz/RbwwSmP/Xz3uBeBm4c9lwWe+58w+Vvk692f9aaF7n8Ycwf+HfBD4Pkpt/cPez4LNPd/wuQJ9+eBbwMfH/ZczsfNTz5LkhoeSpIkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLj/wPetUrxUywHywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_article_sents(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.219544457292887"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding Cosine Similarity Between All Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"Drivers don’t always realize that they may be overpaying for car insurance. If you haven't compared quotes\n",
    "recently, even if you have a low rate, you could still be paying too much. Fortunately, millions of smart drivers have\n",
    "used EverQuote™'s free service to save hundreds on their insurance bills. It’s really no wonder that with so many \n",
    "drivers saving money, EverQuote™ is gaining momentum. EverQuote™ is an efficient source that tries to give consumers\n",
    "the lowest rates with tools you can trust. Just imagine what you could do with the money you save!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2 = \"\"\"President Donald Trump and his Polish counterpart Andrzej Duda were to announce higher US troop levels in Poland \n",
    "on Wednesday, with the main question being whether Washington will defy Russian objections to establish an American \n",
    "base in the NATO country. A senior Trump administration official said the White House meeting would see the two \n",
    "leaders make a significant announcement.\" Whether Trump will risk irritating Moscow with a base or take the simpler \n",
    "option of adding more troops to the current non-permanent force was unclear. Located deep in what used to be \n",
    "Soviet-dominated eastern Europe, Poland is a member of NATO but has long wanted deeper US commitment. Spooked by \n",
    "resurgent Russia's seizing control of territory in Georgia and Ukraine over the last decade, Duda has tried to charm \n",
    "the US president, even touting the idea of Poland building a \"Fort Trump\" to house thousands of US soldiers.\n",
    "Krzysztof Szczerski, an adviser to the Polish president, said the general concept of a \"Fort Trump\" was on the \n",
    "agenda Wednesday and that the US presence \"will increase both in quality as well as quantity.\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>bills</th>\n",
       "      <th>car</th>\n",
       "      <th>compared</th>\n",
       "      <th>consumers</th>\n",
       "      <th>could</th>\n",
       "      <th>drivers</th>\n",
       "      <th>efficient</th>\n",
       "      <th>even</th>\n",
       "      <th>everquote</th>\n",
       "      <th>fortunately</th>\n",
       "      <th>free</th>\n",
       "      <th>gaining</th>\n",
       "      <th>give</th>\n",
       "      <th>hundreds</th>\n",
       "      <th>imagine</th>\n",
       "      <th>insurance</th>\n",
       "      <th>low</th>\n",
       "      <th>lowest</th>\n",
       "      <th>many</th>\n",
       "      <th>may</th>\n",
       "      <th>millions</th>\n",
       "      <th>momentum</th>\n",
       "      <th>money</th>\n",
       "      <th>much</th>\n",
       "      <th>overpaying</th>\n",
       "      <th>paying</th>\n",
       "      <th>quotes</th>\n",
       "      <th>rate</th>\n",
       "      <th>rates</th>\n",
       "      <th>realize</th>\n",
       "      <th>really</th>\n",
       "      <th>recently</th>\n",
       "      <th>save</th>\n",
       "      <th>saving</th>\n",
       "      <th>service</th>\n",
       "      <th>smart</th>\n",
       "      <th>source</th>\n",
       "      <th>still</th>\n",
       "      <th>tools</th>\n",
       "      <th>tries</th>\n",
       "      <th>trust</th>\n",
       "      <th>used</th>\n",
       "      <th>wonder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drivers don’t always realize that they may be overpaying for car insurance.</th>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If you haven't compared quotes\\nrecently, even if you have a low rate, you could still be paying too much.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fortunately, millions of smart drivers have\\nused EverQuote™'s free service to save hundreds on their insurance bills.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215681</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255464</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311536</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It’s really no wonder that with so many \\ndrivers saving money, EverQuote™ is gaining momentum.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.296845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EverQuote™ is an efficient source that tries to give consumers\\nthe lowest rates with tools you can trust.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.324797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Just imagine what you could do with the money you save!</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.472079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      always     bills  \\\n",
       "Drivers don’t always realize that they may be o...  0.403183  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.311536   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                         car  compared  \\\n",
       "Drivers don’t always realize that they may be o...  0.403183  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                    consumers     could  \\\n",
       "Drivers don’t always realize that they may be o...   0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...   0.000000  0.263666   \n",
       "Fortunately, millions of smart drivers have\\nus...   0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...   0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...   0.324797  0.000000   \n",
       "Just imagine what you could do with the money y...   0.000000  0.472079   \n",
       "\n",
       "                                                     drivers  efficient  \\\n",
       "Drivers don’t always realize that they may be o...  0.279128   0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000   0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.215681   0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.250617   0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000   0.324797   \n",
       "Just imagine what you could do with the money y...  0.000000   0.000000   \n",
       "\n",
       "                                                        even  everquote  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000   0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.321538   0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000   0.215681   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000   0.250617   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000   0.224861   \n",
       "Just imagine what you could do with the money y...  0.000000   0.000000   \n",
       "\n",
       "                                                    fortunately      free  \\\n",
       "Drivers don’t always realize that they may be o...     0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...     0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...     0.311536  0.311536   \n",
       "It’s really no wonder that with so many \\ndrive...     0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...     0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...     0.000000  0.000000   \n",
       "\n",
       "                                                    gaining      give  \\\n",
       "Drivers don’t always realize that they may be o...    0.000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...    0.000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...    0.000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...    0.362  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...    0.000  0.324797   \n",
       "Just imagine what you could do with the money y...    0.000  0.000000   \n",
       "\n",
       "                                                    hundreds   imagine  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.311536  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.575696   \n",
       "\n",
       "                                                    insurance       low  \\\n",
       "Drivers don’t always realize that they may be o...   0.330615  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...   0.000000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...   0.255464  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...   0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...   0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...   0.000000  0.000000   \n",
       "\n",
       "                                                      lowest   many       may  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000  0.403183   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.362  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.324797  0.000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000  0.000000   \n",
       "\n",
       "                                                    millions  momentum  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000     0.000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000     0.000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.311536     0.000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000     0.362   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000     0.000   \n",
       "Just imagine what you could do with the money y...  0.000000     0.000   \n",
       "\n",
       "                                                       money      much  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.296845  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.472079  0.000000   \n",
       "\n",
       "                                                    overpaying    paying  \\\n",
       "Drivers don’t always realize that they may be o...    0.403183  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...    0.000000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...    0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...    0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...    0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...    0.000000  0.000000   \n",
       "\n",
       "                                                      quotes      rate  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.321538  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                       rates   realize  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.403183   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.324797  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                    really  recently  \\\n",
       "Drivers don’t always realize that they may be o...   0.000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...   0.000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...   0.000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...   0.362  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...   0.000  0.000000   \n",
       "Just imagine what you could do with the money y...   0.000  0.000000   \n",
       "\n",
       "                                                        save  saving  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000   0.000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000   0.000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.255464   0.000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000   0.362   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000   0.000   \n",
       "Just imagine what you could do with the money y...  0.472079   0.000   \n",
       "\n",
       "                                                     service     smart  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.311536  0.311536   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.000000  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                      source     still  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.321538   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.324797  0.000000   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                       tools     tries  \\\n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.000000   \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   \n",
       "EverQuote™ is an efficient source that tries to...  0.324797  0.324797   \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   \n",
       "\n",
       "                                                       trust      used  wonder  \n",
       "Drivers don’t always realize that they may be o...  0.000000  0.000000   0.000  \n",
       "If you haven't compared quotes\\nrecently, even ...  0.000000  0.000000   0.000  \n",
       "Fortunately, millions of smart drivers have\\nus...  0.000000  0.311536   0.000  \n",
       "It’s really no wonder that with so many \\ndrive...  0.000000  0.000000   0.362  \n",
       "EverQuote™ is an efficient source that tries to...  0.324797  0.000000   0.000  \n",
       "Just imagine what you could do with the money y...  0.000000  0.000000   0.000  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(text):\n",
    "    #tokenize sentences\n",
    "    sentences = sent_tokenize(text, language = 'en')\n",
    "    #set stop words\n",
    "    stops = list(set(stopwords.words('english'))) + list(punctuation)\n",
    "    \n",
    "    #vectorize sentences\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    #creat df for article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "    \n",
    "    #find cosine similarity for all sentences\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 2])"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similarities(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_summary(text):\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    return sents_for_sum\n",
    "#     sents_for_sum = sents_for_sum.tolist().sort()\n",
    "#     print(sents_for_sum)\n",
    "#     sent_list = sent_tokenize(text)\n",
    "#     print(len(sent_list))\n",
    "#     sents = []\n",
    "#     for i in sents_for_sum:\n",
    "#         sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "#     summary = ' '.join(sents)\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 0])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_summary(sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize_all(lst):\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in lst:\n",
    "        allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    return totalvocab_stemmed, totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_stemmed, total_vocab_tokenized = stem_tokenize_all(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7984855"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vocab_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7984855"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7984855 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "#create new df with stemmed vocab as the index and tokenized words as column\n",
    "vocab_df = pd.DataFrame({'words': total_vocab_tokenized}, index = total_vocab_stemmed)\n",
    "print(str(vocab_df.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final</th>\n",
       "      <td>finale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words\n",
       "in          in\n",
       "the        the\n",
       "second  second\n",
       "season  season\n",
       "final   finale"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinkrause/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97064, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time \n",
    "#fit the vectorizer to summaries\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(summaries) \n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of features from the tf-idf matrix\n",
    "features = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'said']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-995064b16a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[0;32m--> 905\u001b[0;31m                         dense_output=dense_output)\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    502\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m            indptr)\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#find cosine similarity \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
